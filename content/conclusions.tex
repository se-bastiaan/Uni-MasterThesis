\chapter{Conclusions}\label{ch:conclusions}

In this thesis we have shown that we can use visual transformers with a linear attention function to tackle an anomaly detection problem using inpainting. Our primary goal was to answer the main research question we asked in the introduction: \textsl{What is the effect on the performance and efficiency of using linear transformers in an inpainting context for anomaly detection when compared to regular transformers?}

To answer this question we have successfully implemented two types of models for inpainting: one using regular transformers and one using linear transformers. To evaluate our models they have been trained on the MVTec AD dataset and the results of the inpainting task were used to perform anomaly detection. This required us to calculate anomaly maps using the MSGMS metric that we were able to compare with the masks provided by the dataset.

The results show that linear transformers are able to outperform the regular transformers in the tasks of segmentation and detection. They require less epochs to be able to converge, however in contrast to what we would have expected the time per epoch is longer for these models.

This should answer our primary research question. However, we think that our results should be viewed with caution. It is possible that parts of our implementation affect the resource usage negatively and therefore the time required to train our models.

For future work our models could be revisited to find the resource usage of the different parts like the data loader and metrics functions. An in-depth analysis of the resource usage of these parts could give a better insight to understand our results.

Another change could be adding augmentations to the data loader to see if this would improve the results. As well as hyperparameter optimisation. It might be possible that the different visual transformers can reach better results with other parameters than those that we re-used from other papers. As well as changing the evaluation and the loss function to find better options that would be better suited for the reconstruction task.

Different approaches using linear transformers could also be explored. For example by using linear transformers for feature extraction in a similar approach like the one introduced in \cite{yu_fastflow_2021}.

Apart from using the linear transformers in another context it is also an option to use fastformers \cite{wu_fastformer_2021} or linformers \cite{wang_linformer_2020} in this context of anomaly detection. This would allow us to evaluate the performance of multiple transformer based approaches to see the effects when compared to regular transformers.