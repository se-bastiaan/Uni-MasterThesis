\chapter{Introduction}\label{ch:introduction}

The detection of deviations in data is an important topic in multiple fields. Examples of these include medical imaging \cite{han_madgan_2021}, surveillance \cite{shashikar_traffic_2017} and manufacturing \cite{susto_anomaly_2017}. These types of anomaly detection aim to find irregularities in different types of images. Using this type of processing we can improve outputs and diagnoses of systems either by automating the process or giving support in manual tasks.

Notably self-supervised anomaly detection is an approach that has been explored recently \cite{li_cutpaste_2021, ali_self-supervised_2020}. This type of machine learning means that we use unlabeled data to train our models. We can use examples of normal situations to be able to reconstruct full images where the anomaly has been removed. This allows us to compare a source image to a reconstruction and find any anomalies if present. This method using image inpainting has been implemented using various methods like generative adversarial networks (GANs) \cite{yeh_semantic_2017} and visual transformers \cite{pirnay_inpainting_2021}.

Transformers \cite{vaswani_attention_2017} are a type of model that is based on self-attention. This attention processes a lot of context, which makes the training of these types of models slow. Therefore alternatives have been explored, like linear transformers \cite{katharopoulos_transformers_2020}, fastformers \cite{wu_fastformer_2021}, linformers \cite{wang_linformer_2020} and performers \cite{choromanski_rethinking_2020}.

Since the linear transformers seem to have some performance improvement over the regular transformers we see an opportunity to use this newer type of transformer in the context of image inpainting for anomaly detection. We propose to use a similar method to the one used by Pirnay et al. \cite{pirnay_inpainting_2021} where we replace the used visual transformer by a visual linear transformer similar to the one introduced by Katharopoulos et al. \cite{katharopoulos_transformers_2020}. Thus we ask:

\textsl{What is the effect on the performance and efficiency of using linear transformers in an inpainting context for anomaly detection when compared to regular transformers?}

To be able to compare the different models we ask subquestions about efficiency and performance:
\begin{itemize}
    % inpainting/anomaly detection/localisation
    \item What is the difference between softmax and linear attention in \textsl{detection} of anomalies across images with different textures and objects?
    \item What is the difference between softmax and linear attention in \textsl{segmentation} of anomalies across images with difference textures and objects?
    % performance/efficiency
    \item How does the \textsl{number of epochs} required to get the best result compare between the softmax and linear attention models across images with different textures and objects?
    \item How does the \textsl{training time} compare between the softmax and linear attention models across images with different textures and objects?
    % linear vs regular
    \item How do our results compare to previous work?
\end{itemize}

We will answer these questions by doing an in-depth analysis of using linear transformers compared to regular transformers.

This thesis is structured as follows: first we will discuss machine learning in general (\ref{sec:prelim:types-of-machine-learning}). We will look at transformers and the different types that we need to answer our research questions (\ref{sec:prelim:transformers}). Next we will introduce the concepts of image inpainting and anomaly detection (\ref{sec:prelim:image-inpainting}, \ref{sec:prelim:anomaly-detection}) followed by measures for image similarity (\ref{sec:prelim:image-similarity}). Consecutively we will discuss related work in the fields of image inpainting and anomaly detection (\ref{sec:relwork:image-inpainting}, \ref{sec:relwork:anomaly-detection}) before explaining how we setup our experiment (\ref{ch:experimental-setup}). Then we will show the results from the experimental setup looking at detection, segmentation (\ref{sec:results:det-seg}) and efficiency (\ref{sec:results:efficiency}). A discussion of the experimental setup and the results follows (\ref{ch:discussion}) and lastly we will draw our conclusions (\ref{ch:conclusions}).