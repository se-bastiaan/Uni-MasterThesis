\chapter{Related Work}\label{ch:related-work}

This section introduces previous research into the two main problems we are dealing with: image inpainting and anomaly detection.

\section{Image inpainting}
\label{sec:relwork:image-inpainting}

Inpainting in images is a subject that was already explored by Bertalmio et a. \cite{bertalmio_image_2000} in 2000. They modeled their algorithm after manual inpainting concepts used by conservators. However, their algorithm still needs user input in the form of a mask of the image sections that have to be inpainted. They also encountered problems filling in larger textured regions.

This is why they followed up their initial paper in 2003 \cite{bertalmio_simultaneous_2003} that combined texture synthesis with their previously introduced structure inpainting algorithm by decomposing the input image. This method thus depends on three different types of methods to achieve it's goals: inpainting, texture synthesis and image decomposition. 

In 2004 Criminisi et al. \cite{criminisi_region_2004} proposed a single algorithm that could fill both structures and textures. They use an algorithm that prioritises patches to fill along a user selected area, these patches are then filled by using the pixels from a similar looking source location. However, this does not handle curved structures very well. They also remark that quantifying the performance of their algorithm is a non-trivial task.

Their work is further extended upon by Bugeau and Bertalmio, by introducing a new algorithm for diffusion and texture synthesis \cite{bugeau_combining_2009}. In later work Bugeau et al. \cite{bugeau_comprehensive_2010} identify three main similarity components: texture synthesis, diffusion and coherence. They try to minimise these components in a new algorithm for inpainting inspired by the PatchMatch algorithm \cite{barnes_patchmatch_2009}. They note that it may be possible that there are no similar patches in the image when the area that needs to be painted is large. Which means that their approach gives poor results for these kinds of situations.

All previously mentioned work still require the section that requires inpainting to be marked prior to applying an algorithm. This non-blind inpainting is addressed by Xie et al. \cite{xie_image_2012} who introduce a deep neural network based approach that continues previous work on denoising \cite{mairal_sparse_2008} and blind inpainting \cite{dong_wavelet_2012}. For this they use stacked denoising autoencoders. However, their method relies on supervised learning and is mostly focused on removing small noise from input images.

An approach that focuses on semantic inpainting of larger regions was introduced by Yeh et al. \cite{yeh_semantic_2017}. Their approach uses a generative adversarial network based model that is trained to give realistic results. The predictions here are also limited by the network and the training procedure. This means that it shows promising results but may not be applicable to more complex structures. This is also true for an approach using a patch-based GAN \cite{demir_patch-based_2018} which focuses on higher resolution images, which is not the case for the context based approach from \cite{pathak_context_2016}.

Another GAN-based approach \cite{yu_generative_2018} introduces an contextual attention layer into a model that uses both local and global loss for the GAN. The contextual attention layer learns where to borrow or copy information to create reconstructions. This especially improves the inpainting of larger regions.

Buiding upon this contextual attention approach both Yu \cite{yu_diverse_2021} and Pirnay \cite{pirnay_inpainting_2021} use transformers for inpainting. Both use positional embeddings. The first approach focuses on realistic reconstructions of landscapes and faces using texture generation. The approach by Pirnay is mainly focused on usage for anomaly detection and only focuses on reconstructing one single type of image, wich would most likely make it unsuitable for the images used by Yu.

\section{Anomaly detection}
\label{sec:relwork:anomaly-detection}

The subject of anomaly detection is very large, since anomalies can be found in all kinds of datasets. In our case we are focusing on anomaly detection in image representations. Most notably related work using the MVTec AD dataset \cite{bergmann_mvtec_2019}.

The MVTec AD dataset contains images that were specifically selected for unsupervised anomaly detection. This allows for easier evaluation of novel approaches and makes it possible to compare models by quantifying the performance. This is illustrated by applying existing methods on the new dataset.

One of these methods by Bergmann \cite{bergmann_improving_2019} uses convolutional autoencoders to segment anomalous sections in images after training a model only on good samples. They use both a per-pixel L2 loss and the structural similarity index (SSIM) to create two models. They show that using SSIM as metric improves their results. For MVTec AD this seems the best performing model but both types of autoencoders fail to reconstruct small details.

A different approach that has more problems getting good results uses GANs. In this case a model by Schlegl et al. \cite{schlegl_unsupervised_2017}. Here the results on MVTec AD have trouble with the images including a lot of variations. The categories that perform better are the bottle and pill images that do not contain any rotations or different shapes.

The last approach that is applicable to all the types of images uses a convolutional neural network for feature discovery. This method by Napoletano et al. \cite{napoletano_anomaly_2018}   was designed for binary classification of images to determine if there is an anomaly or not. To be able to create a course anomaly map the model was applied to smaller patches in the image. This achieves satisfactory results but since the model is applied to the different colour layers separately the anomalies in colour are not detected.

More recently Zavrtanik et al. \cite{zavrtanik_reconstruction_2021} used the MVTec AD dataset for an inpainting approach using a U-net based encoder-decoder network (RIAD). Just like the convolutional autoencoders mentioned previously the loss function uses the SSIM. They combine this with the multi-scale gradient magnitude similarity \cite{xue_gradient_2014} to focus on more image properties.

The RIAD approach generally outperforms all the previously mentioned models. And since it uses an inpainting approach it is also the most similar model compared to the inpainting transformer by Pirnay et al. \cite{pirnay_inpainting_2021} that our work is based upon.

The most recent work with the best results for segmentation and detection we could find using the MVTec AD dataset is \cite{yu_fastflow_2021}. Their approach uses a feature extraction approach that they argue is less complex than  \cite{roth_towards_2021} and \cite{gudovskiy_cflow-ad_2021} which all have a 98\% AUROC for detection and segmentation on the dataset. These approaches are what we consider the current state-of-the-art for the dataset.

% Vergeet niet ook nadelen te benadrukken
% Kijken hoe dingen vergelijken