
@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2021-10-22},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ba_layer_2016,
	title = {Layer {Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	urldate = {2021-10-22},
	journal = {arXiv:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.06450},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2021-10-20},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{cho_learning_2014-1,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2021-10-20},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2021-10-20},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv: 1409.0473},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{xie_image_2012,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'12},
	title = {Image denoising and inpainting with deep neural networks},
	abstract = {We present a novel approach to low-level vision problems that combines sparse coding and deep networks pre-trained with denoising auto-encoder (DA). We propose an alternative training scheme that successfully adapts DA, originally designed for unsupervised feature learning, to the tasks of image denoising and blind inpainting. Our method's performance in the image denoising task is comparable to that of KSVD which is a widely used sparse coding technique. More importantly, in blind image inpainting task, the proposed method provides solutions to some complex problems that have not been tackled before. Specifically, we can automatically remove complex patterns like superimposed text from an image, rather than simple patterns like pixels missing at random. Moreover, the proposed method does not need the information regarding the region that requires inpainting to be given a priori. Experimental results demonstrate the effectiveness of the proposed method in the tasks of image denoising and blind inpainting. We also show that our new training scheme for DA is more effective and can improve the performance of unsupervised feature learning.},
	urldate = {2021-10-18},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 1},
	publisher = {Curran Associates Inc.},
	author = {Xie, Junyuan and Xu, Linli and Chen, Enhong},
	month = dec,
	year = {2012},
	pages = {341--349},
}

@article{wu_visual_2020,
	title = {Visual {Transformers}: {Token}-based {Image} {Representation} and {Processing} for {Computer} {Vision}},
	shorttitle = {Visual {Transformers}},
	url = {http://arxiv.org/abs/2006.03677},
	abstract = {Computer vision has achieved remarkable success by (a) representing images as uniformly-arranged pixel arrays and (b) convolving highly-localized features. However, convolutions treat all image pixels equally regardless of importance; explicitly model all concepts across all images, regardless of content; and struggle to relate spatially-distant concepts. In this work, we challenge this paradigm by (a) representing images as semantic visual tokens and (b) running transformers to densely model token relationships. Critically, our Visual Transformer operates in a semantic token space, judiciously attending to different image parts based on context. This is in sharp contrast to pixel-space transformers that require orders-of-magnitude more compute. Using an advanced training recipe, our VTs significantly outperform their convolutional counterparts, raising ResNet accuracy on ImageNet top-1 by 4.6 to 7 points while using fewer FLOPs and parameters. For semantic segmentation on LIP and COCO-stuff, VT-based feature pyramid networks (FPN) achieve 0.35 points higher mIoU while reducing the FPN module's FLOPs by 6.5x.},
	urldate = {2021-10-18},
	journal = {arXiv:2006.03677 [cs, eess]},
	author = {Wu, Bichen and Xu, Chenfeng and Dai, Xiaoliang and Wan, Alvin and Zhang, Peizhao and Yan, Zhicheng and Tomizuka, Masayoshi and Gonzalez, Joseph and Keutzer, Kurt and Vajda, Peter},
	month = nov,
	year = {2020},
	note = {arXiv: 2006.03677},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Transformer},
}

@article{katharopoulos_transformers_2020,
	title = {Transformers are {RNNs}: {Fast} {Autoregressive} {Transformers} with {Linear} {Attention}},
	shorttitle = {Transformers are {RNNs}},
	url = {http://arxiv.org/abs/2006.16236},
	abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textasciicircum}2{\textbackslash}right)\$ to \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textbackslash}right)\$, where \$N\$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
	urldate = {2021-10-18},
	journal = {arXiv:2006.16236 [cs, stat]},
	author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, François},
	month = aug,
	year = {2020},
	note = {arXiv: 2006.16236},
	keywords = {Attention, Computer Science - Machine Learning, Statistics - Machine Learning, Transformer},
}

@article{yeh_semantic_2017,
	title = {Semantic {Image} {Inpainting} with {Deep} {Generative} {Models}},
	url = {http://arxiv.org/abs/1607.07539},
	abstract = {Semantic image inpainting is a challenging task where large missing regions have to be filled based on the available visual data. Existing methods which extract information from only a single image generally produce unsatisfactory results due to the lack of high level context. In this paper, we propose a novel method for semantic image inpainting, which generates the missing content by conditioning on the available data. Given a trained generative model, we search for the closest encoding of the corrupted image in the latent image manifold using our context and prior losses. This encoding is then passed through the generative model to infer the missing content. In our method, inference is possible irrespective of how the missing content is structured, while the state-of-the-art learning based method requires specific information about the holes in the training phase. Experiments on three datasets show that our method successfully predicts information in large missing regions and achieves pixel-level photorealism, significantly outperforming the state-of-the-art methods.},
	urldate = {2021-10-18},
	journal = {arXiv:1607.07539 [cs]},
	author = {Yeh, Raymond A. and Chen, Chen and Lim, Teck Yian and Schwing, Alexander G. and Hasegawa-Johnson, Mark and Do, Minh N.},
	month = jul,
	year = {2017},
	note = {arXiv: 1607.07539},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Inpainting},
}

@article{doersch_multi-task_2017,
	title = {Multi-task {Self}-{Supervised} {Visual} {Learning}},
	url = {http://arxiv.org/abs/1708.07860},
	abstract = {We investigate methods for combining multiple self-supervised tasks--i.e., supervised tasks where data can be collected without manual labeling--in order to train a single visual representation. First, we provide an apples-to-apples comparison of four different self-supervised tasks using the very deep ResNet-101 architecture. We then combine tasks to jointly train a network. We also explore lasso regularization to encourage the network to factorize the information in its representation, and methods for "harmonizing" network inputs in order to learn a more unified representation. We evaluate all methods on ImageNet classification, PASCAL VOC detection, and NYU depth prediction. Our results show that deeper networks work better, and that combining tasks--even via a naive multi-head architecture--always improves performance. Our best joint network nearly matches the PASCAL performance of a model pre-trained on ImageNet classification, and matches the ImageNet network on NYU depth prediction.},
	urldate = {2021-10-18},
	journal = {arXiv:1708.07860 [cs]},
	author = {Doersch, Carl and Zisserman, Andrew},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.07860},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Self-supervised},
}

@article{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2021-10-18},
	journal = {arXiv:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv: 2010.11929},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Transformer},
}

@article{yu_diverse_2021,
	title = {Diverse {Image} {Inpainting} with {Bidirectional} and {Autoregressive} {Transformers}},
	url = {http://arxiv.org/abs/2104.12335},
	abstract = {Image inpainting is an underdetermined inverse problem, which naturally allows diverse contents to fill up the missing or corrupted regions realistically. Prevalent approaches using convolutional neural networks (CNNs) can synthesize visually pleasant contents, but CNNs suffer from limited perception fields for capturing global features. With image-level attention, transformers enable to model long-range dependencies and generate diverse contents with autoregressive modeling of pixel-sequence distributions. However, the unidirectional attention in autoregressive transformers is suboptimal as corrupted image regions may have arbitrary shapes with contexts from any direction. We propose BAT-Fill, an innovative image inpainting framework that introduces a novel bidirectional autoregressive transformer (BAT) for image inpainting. BAT utilizes the transformers to learn autoregressive distributions, which naturally allows the diverse generation of missing contents. In addition, it incorporates the masked language model like BERT, which enables bidirectionally modeling of contextual information of missing regions for better image completion. Extensive experiments over multiple datasets show that BAT-Fill achieves superior diversity and fidelity in image inpainting qualitatively and quantitatively.},
	urldate = {2021-10-18},
	journal = {arXiv:2104.12335 [cs]},
	author = {Yu, Yingchen and Zhan, Fangneng and Wu, Rongliang and Pan, Jianxiong and Cui, Kaiwen and Lu, Shijian and Ma, Feiying and Xie, Xuansong and Miao, Chunyan},
	month = jun,
	year = {2021},
	note = {arXiv: 2104.12335},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Inpainting, Transformer},
}

@article{zenati_efficient_2019,
	title = {Efficient {GAN}-{Based} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1802.06222},
	abstract = {Generative adversarial networks (GANs) are able to model the complex highdimensional distributions of real-world data, which suggests they could be effective for anomaly detection. However, few works have explored the use of GANs for the anomaly detection task. We leverage recently developed GAN models for anomaly detection, and achieve state-of-the-art performance on image and network intrusion datasets, while being several hundred-fold faster at test time than the only published GAN-based method.},
	urldate = {2021-10-18},
	journal = {arXiv:1802.06222 [cs, stat]},
	author = {Zenati, Houssam and Foo, Chuan Sheng and Lecouat, Bruno and Manek, Gaurav and Chandrasekhar, Vijay Ramaseshan},
	month = may,
	year = {2019},
	note = {arXiv: 1802.06222},
	keywords = {Anomaly detection, Computer Science - Machine Learning, GAN, Statistics - Machine Learning},
}

@article{ngo_fence_2019,
	title = {Fence {GAN}: {Towards} {Better} {Anomaly} {Detection}},
	shorttitle = {Fence {GAN}},
	url = {http://arxiv.org/abs/1904.01209},
	abstract = {Anomaly detection is a classical problem where the aim is to detect anomalous data that do not belong to the normal data distribution. Current state-of-the-art methods for anomaly detection on complex high-dimensional data are based on the generative adversarial network (GAN). However, the traditional GAN loss is not directly aligned with the anomaly detection objective: it encourages the distribution of the generated samples to overlap with the real data and so the resulting discriminator has been found to be ineffective as an anomaly detector. In this paper, we propose simple modifications to the GAN loss such that the generated samples lie at the boundary of the real data distribution. With our modified GAN loss, our anomaly detection method, called Fence GAN (FGAN), directly uses the discriminator score as an anomaly threshold. Our experimental results using the MNIST, CIFAR10 and KDD99 datasets show that Fence GAN yields the best anomaly classification accuracy compared to state-of-the-art methods.},
	urldate = {2021-10-18},
	journal = {arXiv:1904.01209 [cs, stat]},
	author = {Ngo, Cuong Phuc and Winarto, Amadeus Aristo and Li, Connie Kou Khor and Park, Sojeong and Akram, Farhan and Lee, Hwee Kuan},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.01209},
	keywords = {Anomaly detection, Computer Science - Machine Learning, GAN, Statistics - Machine Learning},
}

@article{erfani_high-dimensional_2016,
	title = {High-dimensional and large-scale anomaly detection using a linear one-class {SVM} with deep learning},
	volume = {58},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320316300267},
	doi = {10.1016/j.patcog.2016.03.028},
	abstract = {High-dimensional problem domains pose significant challenges for anomaly detection. The presence of irrelevant features can conceal the presence of anomalies. This problem, known as the ‘curse of dimensionality’, is an obstacle for many anomaly detection techniques. Building a robust anomaly detection model for use in high-dimensional spaces requires the combination of an unsupervised feature extractor and an anomaly detector. While one-class support vector machines are effective at producing decision surfaces from well-behaved feature vectors, they can be inefficient at modelling the variation in large, high-dimensional datasets. Architectures such as deep belief networks (DBNs) are a promising technique for learning robust features. We present a hybrid model where an unsupervised DBN is trained to extract generic underlying features, and a one-class SVM is trained from the features learned by the DBN. Since a linear kernel can be substituted for nonlinear ones in our hybrid model without loss of accuracy, our model is scalable and computationally efficient. The experimental results show that our proposed model yields comparable anomaly detection performance with a deep autoencoder, while reducing its training and testing time by a factor of 3 and 1000, respectively.},
	language = {en},
	urldate = {2021-10-18},
	journal = {Pattern Recognition},
	author = {Erfani, Sarah M. and Rajasegarar, Sutharshan and Karunasekera, Shanika and Leckie, Christopher},
	month = oct,
	year = {2016},
	keywords = {Anomaly detection, Deep belief net, Deep learning, Feature extraction, High-dimensional data, One-class SVM, Outlier detection},
	pages = {121--134},
}

@inproceedings{loganathan_sequence_2018,
	title = {Sequence to {Sequence} {Pattern} {Learning} {Algorithm} for {Real}-{Time} {Anomaly} {Detection} in {Network} {Traffic}},
	doi = {10.1109/CCECE.2018.8447597},
	abstract = {Network intrusions can be modeled as anomalies in network traffic in which the expected order of packets and their attributes deviate from regular traffic. Algorithms that predict the next sequence of events based on previous sequences are a promising avenue for detecting such anomalies. In this paper, we present a novel multi-attribute model for predicting a network packet sequence based on previous packets using a sequence-to-sequence (Seq2Seq) encoder-decoder model. This model is trained on an attack-free dataset to learn the normal sequence of packets in TCP connections and then it is used to detect anomalous packets in TCP traffic. We show that in DARPA 1999 dataset, the proposed multi-attribute Seq2Seq model detects anomalous raw TCP packets which are part of intrusions with 97 \% accuracy. Also, it can detect selected intrusions in real-time with 100\% accuracy and outperforms existing algorithms based on recurrent neural network models such as LSTM.},
	booktitle = {2018 {IEEE} {Canadian} {Conference} on {Electrical} {Computer} {Engineering} ({CCECE})},
	author = {Loganathan, Gobinath and Samarabandu, Jagath and Wang, Xianbin},
	month = may,
	year = {2018},
	note = {ISSN: 2576-7046},
	keywords = {Anomaly detection, Computational modeling, Decoding, Predictive models, Real-time systems, Training, Videos},
	pages = {1--4},
}

@book{mohri_foundations_2012,
	address = {Cambridge, MA, USA},
	series = {Adaptive {Computation} and {Machine} {Learning} series},
	title = {Foundations of {Machine} {Learning}},
	isbn = {978-0-262-01825-8},
	abstract = {Fundamental topics in machine learning are presented along with theoretical and conceptual tools for the discussion and proof of algorithms.},
	language = {en},
	publisher = {MIT Press},
	author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
	editor = {Bach, Francis},
	month = aug,
	year = {2012},
}

@article{katharopoulos_transformers_2020-1,
	title = {Transformers are {RNNs}: {Fast} {Autoregressive} {Transformers} with {Linear} {Attention}},
	shorttitle = {Transformers are {RNNs}},
	url = {http://arxiv.org/abs/2006.16236},
	abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textasciicircum}2{\textbackslash}right)\$ to \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textbackslash}right)\$, where \$N\$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
	urldate = {2021-10-18},
	journal = {arXiv:2006.16236 [cs, stat]},
	author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, François},
	month = aug,
	year = {2020},
	note = {arXiv: 2006.16236},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{prakash_multi-modal_2021,
	title = {Multi-{Modal} {Fusion} {Transformer} for {End}-to-{End} {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2104.09224},
	abstract = {How should representations from complementary sensors be integrated for autonomous driving? Geometry-based sensor fusion has shown great promise for perception tasks such as object detection and motion forecasting. However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in traffic light state can affect the behavior of a vehicle geometrically distant from that traffic light. Geometry alone may therefore be insufficient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that imitation learning policies based on existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global contextual reasoning, such as handling traffic oncoming from multiple directions at uncontrolled intersections. Therefore, we propose TransFuser, a novel Multi-Modal Fusion Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efficacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reducing collisions by 76\% compared to geometry-based fusion.},
	urldate = {2021-10-18},
	journal = {arXiv:2104.09224 [cs]},
	author = {Prakash, Aditya and Chitta, Kashyap and Geiger, Andreas},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.09224},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{chen_generative_nodate,
	title = {Generative {Pretraining} from {Pixels}},
	abstract = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we ﬁnd that a GPT-2 scale model learns strong image representations as measured by linear probing, ﬁne-tuning, and low-data classiﬁcation. On CIFAR-10, we achieve 96.3\% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0\% accuracy with full ﬁnetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0\% top-1 accuracy on a linear probe of our features.},
	language = {en},
	author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeff and Jun, Heewoo and Dhariwal, Prafulla and Luan, David and Sutskever, Ilya},
	pages = {12},
}

@article{tsai_autoencoder-based_2021,
	title = {Autoencoder-based anomaly detection for surface defect inspection},
	volume = {48},
	issn = {1474-0346},
	url = {https://www.sciencedirect.com/science/article/pii/S1474034621000276},
	doi = {10.1016/j.aei.2021.101272},
	abstract = {In this paper, the unsupervised autoencoder learning for automated defect detection in manufacturing is evaluated, where only the defect-free samples are required for the model training. The loss function of a Convolutional Autoencoder (CAE) model only aims at minimizing the reconstruction errors, and makes the representative features widely spread. The proposed CAE in this study incorporates a regularization that improves the feature distribution of defect-free samples within a tight range. It makes the representative feature vectors of all training samples as close as possible to the mean feature vector so that a defect sample in the evaluation stage can generate a distinct distance from the trained center of defect-free samples. The proposed CAE model with regularizations has been tested on a variety of material surfaces, including textural and patterned surfaces in images. The experimental results reveal that the proposed CAE with regularizations significantly outperforms the conventional CAE for defect detection applications in the industry.},
	language = {en},
	urldate = {2021-10-18},
	journal = {Advanced Engineering Informatics},
	author = {Tsai, Du-Ming and Jen, Po-Hao},
	month = apr,
	year = {2021},
	keywords = {Anomaly detection, Autoencoders, Defect inspection, Machine vision},
	pages = {101272},
}

@article{xie_semisupervised_2021,
	title = {Semisupervised {Training} of {Deep} {Generative} {Models} for {High}-{Dimensional} {Anomaly} {Detection}},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2021.3095150},
	abstract = {Abnormal behaviors in industrial systems may be early warnings on critical events that may cause severe damages to facilities and security. Thus, it is important to detect abnormal behaviors accurately and timely. However, the anomaly detection problem is hard to solve in practice, mainly due to the rareness and the expensive cost to get the labels of the anomalies. Deep generative models parameterized by neural networks have achieved state-of-the-art performance in practice for many unsupervised and semisupervised learning tasks. We present a new deep generative model, Latent Enhanced regression/classification Deep Generative Model (LEDGM), for the anomaly detection problem with multidimensional data. Instead of using two-stage decoupled models, we adopt an end-to-end learning paradigm. Instead of conditioning the latent on the class label, LEDGM conditions the label prediction on the learned latent so that the optimization goal is more in favor of better anomaly detection than better reconstruction that the previously proposed deep generative models have been trained for. Experimental results on several synthetic and real-world small- and large-scale datasets demonstrate that LEDGM can achieve improved anomaly detection performance on multidimensional data with very sparse labels. The results also suggest that both labeled anomalies and labeled normal are valuable for semisupervised learning. Generally, our results show that better performance can be achieved with more labeled data. The ablation experiments show that both the original input and the learned latent provide meaningful information for LEDGM to achieve high performance.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Xie, Qin and Zhang, Peng and Yu, Boseon and Choi, Jaesik},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Anomaly detection, Data models, Generative adversarial networks, Generators, Semisupervised learning, Training, Unsupervised learning, deep generative models, semisupervised learning, variational autoencoder (VAE).},
	pages = {1--10},
}

@inproceedings{luong_effective_2015,
	address = {Lisbon, Portugal},
	title = {Effective {Approaches} to {Attention}-based {Neural} {Machine} {Translation}},
	url = {http://aclweb.org/anthology/D15-1166},
	doi = {10.18653/v1/D15-1166},
	language = {en},
	urldate = {2021-10-18},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Luong, Thang and Pham, Hieu and Manning, Christopher D.},
	year = {2015},
	pages = {1412--1421},
}

@article{pirnay_inpainting_2021,
	title = {Inpainting {Transformer} for {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2104.13897},
	abstract = {Anomaly detection in computer vision is the task of identifying images which deviate from a set of normal images. A common approach is to train deep convolutional autoencoders to inpaint covered parts of an image and compare the output with the original image. By training on anomaly-free samples only, the model is assumed to not being able to reconstruct anomalous regions properly. For anomaly detection by inpainting we suggest it to be beneficial to incorporate information from potentially distant regions. In particular we pose anomaly detection as a patch-inpainting problem and propose to solve it with a purely self-attention based approach discarding convolutions. The proposed Inpainting Transformer (InTra) is trained to inpaint covered patches in a large sequence of image patches, thereby integrating information across large regions of the input image. When training from scratch, in comparison to other methods not using extra training data, InTra achieves results on par with the current state-of-the-art on the MVTec AD dataset for detection and surpassing them on segmentation.},
	urldate = {2021-10-18},
	journal = {arXiv:2104.13897 [cs]},
	author = {Pirnay, Jonathan and Chai, Keng},
	month = sep,
	year = {2021},
	note = {arXiv: 2104.13897},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2021-10-18},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Attention, Computer Science - Computation and Language, Computer Science - Machine Learning, Transformer},
}

@misc{brownlee_what_2017,
	title = {What is {Teacher} {Forcing} for {Recurrent} {Neural} {Networks}?},
	url = {https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/},
	abstract = {Teacher forcing is a method for quickly and efficiently training recurrent neural network models that use the ground truth from […]},
	language = {en-US},
	urldate = {2021-10-18},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = dec,
	year = {2017},
	keywords = {Teacher forcing},
}
