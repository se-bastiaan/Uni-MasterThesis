
@inproceedings{zhang_gradient_2017,
	title = {Gradient magnitude similarity deviation on multiple scales for color image quality assessment},
	doi = {10.1109/ICASSP.2017.7952357},
	abstract = {Recently, various image quality assessment (IQA) metrics based on gradient similarity have been developed. In this paper, we extend the work of gradient magnitude similarity deviation (GMSD) and propose a more efficient metric. First, a novel similarity index is proposed, which gives the flexibility to tune the masking parameter to more closely match the human vision system (HVS). Then, we propose a multi-scale GMSD method by incorporating scores of luminance distortion at different scales. Furthermore, a method for measuring chromatic distortions in YIQ color space based on our metric is proposed. The final IQA index, MS-GMSDc, is obtained by combining luminance and chrominance scores. Experimental results on four comprehensive datasets clearly show that, compared with 14 state-of-the-art IQA methods, our method achieves the best performance for both grayscale and chromatic image assessment.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Zhang, Bo and Sander, Pedro V. and Bermak, Amine},
	month = mar,
	year = {2017},
	note = {ISSN: 2379-190X},
	keywords = {Chromatic Distortion, Color, Distortion, Distortion measurement, Gradient Magnitude Similarity, Gray-scale, Image Quality Assessment (IQA), Image quality, Indexes, Multi-scale},
	pages = {1253--1257},
}

@article{wang_image_2004,
	title = {Image {Quality} {Assessment}: {From} {Error} {Visibility} to {Structural} {Similarity}},
	volume = {13},
	issn = {1057-7149},
	shorttitle = {Image {Quality} {Assessment}},
	url = {http://ieeexplore.ieee.org/document/1284395/},
	doi = {10.1109/TIP.2003.819861},
	language = {en},
	number = {4},
	urldate = {2022-08-01},
	journal = {IEEE Transactions on Image Processing},
	author = {Wang, Z. and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
	month = apr,
	year = {2004},
	pages = {600--612},
}

@article{xue_gradient_2014,
	title = {Gradient {Magnitude} {Similarity} {Deviation}: {A} {Highly} {Efficient} {Perceptual} {Image} {Quality} {Index}},
	volume = {23},
	issn = {1941-0042},
	shorttitle = {Gradient {Magnitude} {Similarity} {Deviation}},
	doi = {10.1109/TIP.2013.2293423},
	abstract = {It is an important task to faithfully evaluate the perceptual quality of output images in many applications, such as image compression, image restoration, and multimedia streaming. A good image quality assessment (IQA) model should not only deliver high quality prediction accuracy, but also be computationally efficient. The efficiency of IQA metrics is becoming particularly important due to the increasing proliferation of high-volume visual data in high-speed networks. We present a new effective and efficient IQA model, called gradient magnitude similarity deviation (GMSD). The image gradients are sensitive to image distortions, while different local structures in a distorted image suffer different degrees of degradations. This motivates us to explore the use of global variation of gradient based local quality map for overall image quality prediction. We find that the pixel-wise gradient magnitude similarity (GMS) between the reference and distorted images combined with a novel pooling strategy-the standard deviation of the GMS map-can predict accurately perceptual image quality. The resulting GMSD algorithm is much faster than most state-of-the-art IQA methods, and delivers highly competitive prediction accuracy. MATLAB source code of GMSD can be downloaded at http://www4.comp.polyu.edu.hk/ cslzhang/IQA/GMSD/GMSD.htm.},
	number = {2},
	journal = {IEEE Transactions on Image Processing},
	author = {Xue, Wufeng and Zhang, Lei and Mou, Xuanqin and Bovik, Alan C.},
	month = feb,
	year = {2014},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Accuracy, Computational modeling, Degradation, Gradient magnitude similarity, Image coding, Image quality, Indexes, Nonlinear distortion, full reference, image quality assessment, loss, standard deviation pooling},
	pages = {684--695},
}

@article{napoletano_anomaly_2018,
	title = {Anomaly {Detection} in {Nanofibrous} {Materials} by {CNN}-{Based} {Self}-{Similarity}},
	volume = {18},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/18/1/209},
	doi = {10.3390/s18010209},
	abstract = {Automatic detection and localization of anomalies in nanofibrous materials help to reduce the cost of the production process and the time of the post-production visual inspection process. Amongst all the monitoring methods, those exploiting Scanning Electron Microscope (SEM) imaging are the most effective. In this paper, we propose a region-based method for the detection and localization of anomalies in SEM images, based on Convolutional Neural Networks (CNNs) and self-similarity. The method evaluates the degree of abnormality of each subregion of an image under consideration by computing a CNN-based visual similarity with respect to a dictionary of anomaly-free subregions belonging to a training set. The proposed method outperforms the state of the art.},
	language = {en},
	number = {1},
	urldate = {2022-06-15},
	journal = {Sensors},
	author = {Napoletano, Paolo and Piccoli, Flavio and Schettini, Raimondo},
	month = jan,
	year = {2018},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {anomaly detection, convolutional neural networks, defect detection, industrial quality inspection, nanofibrous materials, quality control},
	pages = {209},
}

@article{bottger_real-time_2016,
	title = {Real-time texture error detection on textured surfaces with compressed sensing},
	volume = {26},
	issn = {1555-6212},
	url = {https://doi.org/10.1134/S1054661816010053},
	doi = {10.1134/S1054661816010053},
	abstract = {We present a real-time approach to detect and localise defects in grey-scale textures within a Compressed Sensing framework. Inspired by recent results in texture classification, we use compressed local grey-scale patches for texture description. In a first step, a Gaussian Mixture model is trained with the features extracted from a handful of defect-free texture samples. In a second step, the novelty detection of texture samples is performed by comparing each pixel to the likelihood obtained in the training process. The inspection stage is embedded into a multi-scale framework to enable real-time defect detection and localisation. The performance of compressed grey-scale patches for texture error detection is evaluated on two independent datasets. The proposed method is able to outperform the performance of non-compressed grey-scale patches in terms of accuracy and speed.},
	language = {en},
	number = {1},
	urldate = {2022-06-08},
	journal = {Pattern Recognition and Image Analysis},
	author = {Böttger, T. and Ulrich, M.},
	month = jan,
	year = {2016},
	keywords = {compressed sensing, texture inspection},
	pages = {88--94},
}

@misc{noauthor_flush_nodate,
	title = {Flush {Cache} - {Google} {Public} {DNS}},
	url = {https://dns.google/cache},
	urldate = {2022-06-08},
}

@inproceedings{bergmann_improving_2019,
	title = {Improving {Unsupervised} {Defect} {Segmentation} by {Applying} {Structural} {Similarity} to {Autoencoders}},
	url = {http://arxiv.org/abs/1807.02011},
	doi = {10.5220/0007364503720380},
	abstract = {Convolutional autoencoders have emerged as popular methods for unsupervised defect segmentation on image data. Most commonly, this task is performed by thresholding a pixel-wise reconstruction error based on an \${\textbackslash}ell{\textasciicircum}p\$ distance. This procedure, however, leads to large residuals whenever the reconstruction encompasses slight localization inaccuracies around edges. It also fails to reveal defective regions that have been visually altered when intensity values stay roughly consistent. We show that these problems prevent these approaches from being applied to complex real-world scenarios and that it cannot be easily avoided by employing more elaborate architectures such as variational or feature matching autoencoders. We propose to use a perceptual loss function based on structural similarity which examines inter-dependencies between local image regions, taking into account luminance, contrast and structural information, instead of simply comparing single pixel values. It achieves significant performance gains on a challenging real-world dataset of nanofibrous materials and a novel dataset of two woven fabrics over the state of the art approaches for unsupervised defect segmentation that use pixel-wise reconstruction error metrics.},
	urldate = {2022-06-03},
	booktitle = {Proceedings of the 14th {International} {Joint} {Conference} on {Computer} {Vision}, {Imaging} and {Computer} {Graphics} {Theory} and {Applications}},
	author = {Bergmann, Paul and Löwe, Sindy and Fauser, Michael and Sattlegger, David and Steger, Carsten},
	year = {2019},
	note = {arXiv:1807.02011 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {372--380},
}

@inproceedings{schlegl_unsupervised_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Unsupervised {Anomaly} {Detection} with {Generative} {Adversarial} {Networks} to {Guide} {Marker} {Discovery}},
	isbn = {978-3-319-59050-9},
	doi = {10.1007/978-3-319-59050-9_12},
	abstract = {Obtaining models that capture imaging markers relevant for disease progression and treatment monitoring is challenging. Models are typically based on large amounts of data with annotated examples of known markers aiming at automating detection. High annotation effort and the limitation to a vocabulary of known markers limit the power of such approaches. Here, we perform unsupervised learning to identify anomalies in imaging data as candidates for markers. We propose AnoGAN, a deep convolutional generative adversarial network to learn a manifold of normal anatomical variability, accompanying a novel anomaly scoring scheme based on the mapping from image space to a latent space. Applied to new data, the model labels anomalies, and scores image patches indicating their fit into the learned distribution. Results on optical coherence tomography images of the retina demonstrate that the approach correctly identifies anomalous images, such as images containing retinal fluid or hyperreflective foci.},
	language = {en},
	booktitle = {Information {Processing} in {Medical} {Imaging}},
	publisher = {Springer International Publishing},
	author = {Schlegl, Thomas and Seeböck, Philipp and Waldstein, Sebastian M. and Schmidt-Erfurth, Ursula and Langs, Georg},
	editor = {Niethammer, Marc and Styner, Martin and Aylward, Stephen and Zhu, Hongtu and Oguz, Ipek and Yap, Pew-Thian and Shen, Dinggang},
	year = {2017},
	keywords = {Anomaly Detection, Image Patch, Latent Space, Optical Coherence Tomography, Query Image},
	pages = {146--157},
}

@inproceedings{li_cutpaste_2021,
	title = {{CutPaste}: {Self}-{Supervised} {Learning} for {Anomaly} {Detection} and {Localization}},
	shorttitle = {{CutPaste}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Li_CutPaste_Self-Supervised_Learning_for_Anomaly_Detection_and_Localization_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-06-01},
	author = {Li, Chun-Liang and Sohn, Kihyuk and Yoon, Jinsung and Pfister, Tomas},
	year = {2021},
	pages = {9664--9674},
}

@article{bergmann_mvtec_2019,
	title = {{MVTec} {AD} — {A} {Comprehensive} {Real}-{World} {Dataset} for {Unsupervised} {Anomaly} {Detection}},
	abstract = {The detection of anomalous structures in natural image data is of utmost importance for numerous tasks in the ﬁeld of computer vision. The development of methods for unsupervised anomaly detection requires data on which to train and evaluate new approaches and ideas. We introduce the MVTec Anomaly Detection (MVTec AD) dataset containing 5354 high-resolution color images of different object and texture categories. It contains normal, i.e., defect-free, images intended for training and images with anomalies intended for testing. The anomalies manifest themselves in the form of over 70 different types of defects such as scratches, dents, contaminations, and various structural changes. In addition, we provide pixel-precise ground truth regions for all anomalies. We also conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods based on deep architectures such as convolutional autoencoders, generative adversarial networks, and feature descriptors using pre-trained convolutional neural networks, as well as classical computer vision methods. This initial benchmark indicates that there is considerable room for improvement. To the best of our knowledge, this is the ﬁrst comprehensive, multi-object, multi-defect dataset for anomaly detection that provides pixel-accurate ground truth regions and focuses on real-world applications.},
	language = {en},
	journal = {2019},
	author = {Bergmann, Paul and Fauser, Michael and Sattlegger, David and Steger, Carsten},
	year = {2019},
	pages = {9},
}

@article{zavrtanik_reconstruction_2021,
	title = {Reconstruction by inpainting for visual anomaly detection},
	volume = {112},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320305094},
	doi = {10.1016/j.patcog.2020.107706},
	abstract = {Visual anomaly detection addresses the problem of classification or localization of regions in an image that deviate from their normal appearance. A popular approach trains an auto-encoder on anomaly-free images and performs anomaly detection by calculating the difference between the input and the reconstructed image. This approach assumes that the auto-encoder will be unable to accurately reconstruct anomalous regions. But in practice neural networks generalize well even to anomalies and reconstruct them sufficiently well, thus reducing the detection capabilities. Accurate reconstruction is far less likely if the anomaly pixels were not visible to the auto-encoder. We thus cast anomaly detection as a self-supervised reconstruction-by-inpainting problem. Our approach (RIAD) randomly removes partial image regions and reconstructs the image from partial inpaintings, thus addressing the drawbacks of auto-enocoding methods. RIAD is extensively evaluated on several benchmarks and sets a new state-of-the art on a recent highly challenging anomaly detection benchmark.},
	language = {en},
	urldate = {2022-05-30},
	journal = {Pattern Recognition},
	author = {Zavrtanik, Vitjan and Kristan, Matej and Skočaj, Danijel},
	month = apr,
	year = {2021},
	keywords = {Anomaly detection, CNN, Inpainting, Video anomaly detection},
	pages = {107706},
}

@misc{noauthor_anomaly_nodate,
	title = {Anomaly detection of defects on concrete structures with the convolutional autoencoder {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S1474034620300744?token=5D087C80EC797F7C87CD8D6493FBBEC96CFA72EB95CCC0657DAA79EA12344995C401A4C2F24B7BE96D01600E6BE83446&originRegion=eu-west-1&originCreation=20220530124240},
	language = {en},
	urldate = {2022-05-30},
	doi = {10.1016/j.aei.2020.101105},
}

@article{chow_anomaly_2020,
	title = {Anomaly detection of defects on concrete structures with the convolutional autoencoder},
	volume = {45},
	issn = {1474-0346},
	url = {https://www.sciencedirect.com/science/article/pii/S1474034620300744},
	doi = {10.1016/j.aei.2020.101105},
	abstract = {This paper reports the application of deep learning for implementing the anomaly detection of defects on concrete structures, so as to facilitate the visual inspection of civil infrastructure. A convolutional autoencoder was trained as a reconstruction-based model, with the defect-free images, to rapidly and reliably detect defects from the large volume of image datasets. This training process was in the unsupervised mode, with no label needed, thereby requiring no prior knowledge and saving an enormous amount of time for label preparation. The built anomaly detector favors minimizing the reconstruction errors of defect-free images, which renders high reconstruction errors of defects, in turn, detecting the location of defects. The assessment shows that the proposed anomaly detection technique is robust and adaptable to defects on wide ranges of scales. Comparison was also made with the segmentation results produced by other automatic classical methods, revealing that the results made by the anomaly map outperform other segmentation methods, in terms of precision, recall, F1 measure and F2 measure, without severe under- and over-segmentation. Further, instead of merely being a binary map, each pixel of the anomaly map is represented by the anomaly score, which acts as a risk indicator for alerting inspectors, wherever defects on concrete structures are detected.},
	language = {en},
	urldate = {2022-05-30},
	journal = {Advanced Engineering Informatics},
	author = {Chow, J. K. and Su, Z. and Wu, J. and Tan, P. S. and Mao, X. and Wang, Y. H.},
	month = aug,
	year = {2020},
	keywords = {Anomaly detection, Concrete structure, Convolutional autoencoder, Cracking, Spalling, Unsupervised learning},
	pages = {101105},
}

@article{chow_anomaly_2020-1,
	title = {Anomaly detection of defects on concrete structures with the convolutional autoencoder},
	volume = {45},
	issn = {1474-0346},
	url = {https://www.sciencedirect.com/science/article/pii/S1474034620300744},
	doi = {10.1016/j.aei.2020.101105},
	abstract = {This paper reports the application of deep learning for implementing the anomaly detection of defects on concrete structures, so as to facilitate the visual inspection of civil infrastructure. A convolutional autoencoder was trained as a reconstruction-based model, with the defect-free images, to rapidly and reliably detect defects from the large volume of image datasets. This training process was in the unsupervised mode, with no label needed, thereby requiring no prior knowledge and saving an enormous amount of time for label preparation. The built anomaly detector favors minimizing the reconstruction errors of defect-free images, which renders high reconstruction errors of defects, in turn, detecting the location of defects. The assessment shows that the proposed anomaly detection technique is robust and adaptable to defects on wide ranges of scales. Comparison was also made with the segmentation results produced by other automatic classical methods, revealing that the results made by the anomaly map outperform other segmentation methods, in terms of precision, recall, F1 measure and F2 measure, without severe under- and over-segmentation. Further, instead of merely being a binary map, each pixel of the anomaly map is represented by the anomaly score, which acts as a risk indicator for alerting inspectors, wherever defects on concrete structures are detected.},
	language = {en},
	urldate = {2022-05-30},
	journal = {Advanced Engineering Informatics},
	author = {Chow, J. K. and Su, Z. and Wu, J. and Tan, P. S. and Mao, X. and Wang, Y. H.},
	month = aug,
	year = {2020},
	keywords = {Anomaly detection, Concrete structure, Convolutional autoencoder, Cracking, Spalling, Unsupervised learning},
	pages = {101105},
}

@inproceedings{yu_diverse_2021,
	address = {Virtual Event China},
	title = {Diverse {Image} {Inpainting} with {Bidirectional} and {Autoregressive} {Transformers}},
	isbn = {978-1-4503-8651-7},
	url = {https://dl.acm.org/doi/10.1145/3474085.3475436},
	doi = {10.1145/3474085.3475436},
	language = {en},
	urldate = {2022-05-25},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Yu, Yingchen and Zhan, Fangneng and Wu, Rongliang and Pan, Jianxiong and Cui, Kaiwen and Lu, Shijian and Ma, Feiying and Xie, Xuansong and Miao, Chunyan},
	month = oct,
	year = {2021},
	pages = {69--78},
}

@techreport{pathak_context_2016,
	title = {Context {Encoders}: {Feature} {Learning} by {Inpainting}},
	shorttitle = {Context {Encoders}},
	url = {http://arxiv.org/abs/1604.07379},
	abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
	number = {arXiv:1604.07379},
	urldate = {2022-05-25},
	institution = {arXiv},
	author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
	month = nov,
	year = {2016},
	doi = {10.48550/arXiv.1604.07379},
	note = {arXiv:1604.07379 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@article{dong_wavelet_2012,
	title = {Wavelet frame based blind image inpainting},
	volume = {32},
	issn = {10635203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520311000765},
	doi = {10.1016/j.acha.2011.06.001},
	abstract = {Image inpainting has been widely used in practice to repair damaged/missing pixels of given images. Most of the existing inpainting techniques require knowing beforehand where those damaged pixels are, either given as a priori or detected by some preprocessing. However, in certain applications, such information neither is available nor can be reliably pre-detected, e.g. removing random-valued impulse noise from images or removing certain scratches from archived photographs. This paper introduces a blind inpainting model to solve this type of problems, i.e., a model of simultaneously identifying and recovering damaged pixels of the given image. A tight frame based regularization approach is developed in this paper for such blind inpainting problems, and the resulted minimization problem is solved by the split Bregman algorithm ﬁrst proposed by Goldstein and Osher (2009) [1]. The proposed blind inpainting method is applied to various challenging image restoration tasks, including recovering images that are blurry and damaged by scratches and removing image noise mixed with both Gaussian and randomvalued impulse noise. The experiments show that our method is compared favorably against many available two-staged methods in these applications.},
	language = {en},
	number = {2},
	urldate = {2022-05-25},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Dong, Bin and Ji, Hui and Li, Jia and Shen, Zuowei and Xu, Yuhong},
	month = mar,
	year = {2012},
	pages = {268--279},
}

@article{mairal_sparse_2008,
	title = {Sparse {Representation} for {Color} {Image} {Restoration}},
	volume = {17},
	issn = {1941-0042},
	doi = {10.1109/TIP.2007.911828},
	abstract = {Sparse representations of signals have drawn considerable interest in recent years. The assumption that natural signals, such as images, admit a sparse decomposition over a redundant dictionary leads to efficient algorithms for handling such sources of data. In particular, the design of well adapted dictionaries for images has been a major challenge. The K-SVD has been recently proposed for this task and shown to perform very well for various grayscale image processing tasks. In this paper, we address the problem of learning dictionaries for color images and extend the K-SVD-based grayscale image denoising algorithm that appears in . This work puts forward ways for handling nonhomogeneous noise and missing information, paving the way to state-of-the-art results in applications such as color image denoising, demosaicing, and inpainting, as demonstrated in this paper.},
	number = {1},
	journal = {IEEE Transactions on Image Processing},
	author = {Mairal, Julien and Elad, Michael and Sapiro, Guillermo},
	month = jan,
	year = {2008},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Color, Color processing, Dictionaries, Gray-scale, Image denoising, Image processing, Image restoration, Iterative algorithms, Matching pursuit algorithms, Noise reduction, Signal processing, demosaicing, denoising, image decomposition, image processing, image representations, inpainting, sparse representation},
	pages = {53--69},
}

@article{bugeau_comprehensive_2010,
	title = {A {Comprehensive} {Framework} for {Image} {Inpainting}},
	volume = {19},
	issn = {1941-0042},
	doi = {10.1109/TIP.2010.2049240},
	abstract = {Inpainting is the art of modifying an image in a form that is not detectable by an ordinary observer. There are numerous and very different approaches to tackle the inpainting problem, though as explained in this paper, the most successful algorithms are based upon one or two of the following three basic techniques: copy-and-paste texture synthesis, geometric partial differential equations (PDEs), and coherence among neighboring pixels. We combine these three building blocks in a variational model, and provide a working algorithm for image inpainting trying to approximate the minimum of the proposed energy functional. Our experiments show that the combination of all three terms of the proposed energy works better than taking each term separately, and the results obtained are within the state-of-the-art.},
	number = {10},
	journal = {IEEE Transactions on Image Processing},
	author = {Bugeau, Aurélie and Bertalmío, Marcelo and Caselles, Vicent and Sapiro, Guillermo},
	month = oct,
	year = {2010},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Art, Brightness, Coherence, Greedy algorithms, Image inpainting, Image processing, Image restoration, Permission, Pixel, Predictive models, Probability distribution, partial differential equations (PDEs), texture synthesis, variational models},
	pages = {2634--2645},
}

@inproceedings{bugeau_combining_2009,
	address = {Portugal},
	title = {Combining {Texture} {Synthesis} and {Diffusion} for {Image} {Inpainting}.},
	url = {https://hal.archives-ouvertes.fr/hal-00551587},
	abstract = {Image inpainting or image completion consists in filling in the missing data of an image in a visually plausible way. Many works on this subject have been proposed these recent years. They can mainly be decomposed into two groups: geometric methods and texture synthesis methods. Texture synthesis methods work best with images containing only textures while geometric approaches are limited to smooth images containing strong edges. In this paper, we first present an extended state of the art. Then a new algorithm dedicated to both types of images is introduced. The basic idea is to decompose the original image into a structure and a texture image. Each of them is then filled in with some extensions of one of the best methods from the literature. A comparison with some existing methods on different natural images shows the strength of the proposed approach.},
	urldate = {2022-05-23},
	booktitle = {{VISAPP} 2009 - {Proceedings} of the {Fourth} {International} {Conference} on {Computer} {Vision} {Theory} and {Applications}},
	author = {Bugeau, Aurélie and Bertalmio, Marcelo},
	year = {2009},
	pages = {26--33},
}

@inproceedings{le_meur_examplar-based_2011,
	title = {Examplar-based inpainting based on local geometry},
	doi = {10.1109/ICIP.2011.6116441},
	abstract = {In this paper, we propose a novel inpainting algorithm combining the advantages of PDE-based schemes and examplar-based approaches. The proposed algorithm relies on the use of structure tensors to define the filling order priority and template matching. The structure tensors are computed in a hierarchic manner whereas the template matching is based on a K-nearest neighbor algorithm. The value K is adaptively set in function of the local texture information. Compared to two state of the art approaches, the proposed method provides more coherent results.},
	booktitle = {2011 18th {IEEE} {International} {Conference} on {Image} {Processing}},
	author = {Le Meur, Olivier and Gautier, Josselin and Guillemot, Christine},
	month = sep,
	year = {2011},
	note = {ISSN: 2381-8549},
	keywords = {Conferences, Eigenvalues and eigenfunctions, Geometry, Image processing, Robustness, Tensile stress, Vectors, examplar-based, inpainting, tensor},
	pages = {3401--3404},
}

@article{criminisi_region_2004,
	title = {Region filling and object removal by exemplar-based image inpainting},
	volume = {13},
	issn = {1941-0042},
	doi = {10.1109/TIP.2004.833105},
	abstract = {A new algorithm is proposed for removing large objects from digital images. The challenge is to fill in the hole that is left behind in a visually plausible way. In the past, this problem has been addressed by two classes of algorithms: 1) "texture synthesis" algorithms for generating large image regions from sample textures and 2) "inpainting" techniques for filling in small image gaps. The former has been demonstrated for "textures"-repeating two-dimensional patterns with some stochasticity; the latter focus on linear "structures" which can be thought of as one-dimensional patterns, such as lines and object contours. This paper presents a novel and efficient algorithm that combines the advantages of these two approaches. We first note that exemplar-based texture synthesis contains the essential process required to replicate both texture and structure; the success of structure propagation, however, is highly dependent on the order in which the filling proceeds. We propose a best-first algorithm in which the confidence in the synthesized pixel values is propagated in a manner similar to the propagation of information in inpainting. The actual color values are computed using exemplar-based synthesis. In this paper, the simultaneous propagation of texture and structure information is achieved by a single , efficient algorithm. Computational efficiency is achieved by a block-based sampling process. A number of examples on real and synthetic images demonstrate the effectiveness of our algorithm in removing large occluding objects, as well as thin scratches. Robustness with respect to the shape of the manually selected target region is also demonstrated. Our results compare favorably to those obtained by existing techniques.},
	number = {9},
	journal = {IEEE Transactions on Image Processing},
	author = {Criminisi, A. and Perez, P. and Toyama, K.},
	month = sep,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Computational efficiency, Digital images, Filling, Humans, Image generation, Image sampling, Robustness, Shape, Two dimensional displays, Water resources},
	pages = {1200--1212},
}

@article{bertalmio_simultaneous_2003,
	title = {Simultaneous structure and texture image inpainting},
	volume = {12},
	issn = {1941-0042},
	doi = {10.1109/TIP.2003.815261},
	abstract = {An algorithm for the simultaneous filling-in of texture and structure in regions of missing image information is presented in this paper. The basic idea is to first decompose the image into the sum of two functions with different basic characteristics, and then reconstruct each one of these functions separately with structure and texture filling-in algorithms. The first function used in the decomposition is of bounded variation, representing the underlying image structure, while the second function captures the texture and possible noise. The region of missing information in the bounded variation image is reconstructed using image inpainting algorithms, while the same region in the texture image is filled-in with texture synthesis techniques. The original image is then reconstructed adding back these two sub-images. The novel contribution of this paper is then in the combination of these three previously developed components, image decomposition with inpainting and texture synthesis, which permits the simultaneous use of filling-in algorithms that are suited for different image characteristics. Examples on real images show the advantages of this proposed approach.},
	number = {8},
	journal = {IEEE Transactions on Image Processing},
	author = {Bertalmio, M. and Vese, L. and Sapiro, G. and Osher, S.},
	month = aug,
	year = {2003},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Engineering profession, Filling, Image coding, Image communication, Image decomposition, Image processing, Image reconstruction, Image restoration, Switches},
	pages = {882--889},
}

@article{wei_image_2022,
	title = {Image {Inpainting} via {Context} {Discriminator} and {U}-{Net}},
	volume = {2022},
	issn = {1563-5147, 1024-123X},
	url = {https://www.hindawi.com/journals/mpe/2022/7328045/},
	doi = {10.1155/2022/7328045},
	abstract = {Image inpainting is one of the research hotspots in the field of computer vision and image processing. The image inpainting methods based on deep learning models had made some achievements, but it is difficult to achieve ideal results when dealing with images with the relationship between global and local attributions. In particular, when repairing a large area of image defects, the semantic rationality, structural coherence, and detail accuracy of results need to be improved. In view of the existing shortcomings, this study has proposed an improved image inpainting model based on a fully convolutional neural network and generative countermeasure network. A novel image inpainting algorithm via network has been proposed as a generator to repair the defect image, and structural similarity had introduced as the reconstruction loss of image inpainting to supervise and guide model learning from the perspective of the human visual system to improve the effect of image inpainting. The improved global and local context discriminator networks had used as context discriminators to judge the authenticity of the repair results. At the same time, combined with the adversarial loss, a joint loss has proposed for the training of the supervision model, which makes the content of the real and natural repair area and has attribute consistency with the whole image. To verify the effectiveness of the proposed image inpainting model, the image inpainting effect is compared with the current mainstream image inpainting algorithm on the CelebA-HQ dataset based on subjective and objective indicators. The experimental results show that the proposed method has made progress in semantic rationality, structural coherence, and detail accuracy. The proposed model has a better understanding of the high-level semantics of image, and a more accurate grasp of context and detailed information.},
	language = {en},
	urldate = {2022-05-11},
	journal = {Mathematical Problems in Engineering},
	author = {Wei, Ruifang and Wu, Yukun},
	editor = {Kawulok, Michal},
	month = may,
	year = {2022},
	pages = {1--12},
}

@article{yu_free-form_2019,
	title = {Free-{Form} {Image} {Inpainting} with {Gated} {Convolution}},
	url = {http://arxiv.org/abs/1806.03589},
	abstract = {We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: https://github.com/JiahuiYu/generative\_inpainting},
	urldate = {2022-04-27},
	journal = {arXiv:1806.03589 [cs]},
	author = {Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas},
	month = oct,
	year = {2019},
	note = {arXiv: 1806.03589
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@article{yu_generative_2018,
	title = {Generative {Image} {Inpainting} with {Contextual} {Attention}},
	url = {http://arxiv.org/abs/1801.07892},
	abstract = {Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feed-forward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones. Code, demo and models are available at: https://github.com/JiahuiYu/generative\_inpainting.},
	urldate = {2022-04-27},
	journal = {arXiv:1801.07892 [cs]},
	author = {Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S.},
	month = mar,
	year = {2018},
	note = {arXiv: 1801.07892
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {Generative} {Image} {Inpainting} with {Contextual} {Attention}},
	url = {https://paperswithcode.com/paper/generative-image-inpainting-with-contextual},
	abstract = {Implemented in 28 code libraries.},
	language = {en},
	urldate = {2022-04-27},
}

@article{yu_free-form_nodate,
	title = {Free-{Form} {Image} {Inpainting} {With} {Gated} {Convolution}},
	language = {en},
	author = {Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S},
	pages = {10},
}

@article{yu_free-form_nodate-1,
	title = {Free-{Form} {Image} {Inpainting} {With} {Gated} {Convolution}},
	language = {en},
	author = {Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S},
	pages = {10},
}

@inproceedings{bertalmio_image_2000,
	address = {USA},
	series = {{SIGGRAPH} '00},
	title = {Image inpainting},
	isbn = {978-1-58113-208-3},
	url = {https://doi.org/10.1145/344779.344972},
	doi = {10.1145/344779.344972},
	abstract = {Inpainting, the technique of modifying an image in an undetectable form, is as ancient as art itself. The goals and applications of inpainting are numerous, from the restoration of damaged paintings and photographs to the removal/replacement of selected objects. In this paper, we introduce a novel algorithm for digital inpainting of still images that attempts to replicate the basic techniques used by professional restorators. After the user selects the regions to be restored, the algorithm automatically fills-in these regions with information surrounding them. The fill-in is done in such a way that isophote lines arriving at the regions' boundaries are completed inside. In contrast with previous approaches, the technique here introduced does not require the user to specify where the novel information comes from. This is automatically done (and in a fast way), thereby allowing to simultaneously fill-in numerous regions containing completely different structures and surrounding backgrounds. In addition, no limitations are imposed on the topology of the region to be inpainted. Applications of this technique include the restoration of old photographs and damaged film; removal of superimposed text like dates, subtitles, or publicity; and the removal of entire objects from the image like microphones or wires in special effects.},
	urldate = {2022-03-30},
	booktitle = {Proceedings of the 27th annual conference on {Computer} graphics and interactive techniques},
	publisher = {ACM Press/Addison-Wesley Publishing Co.},
	author = {Bertalmio, Marcelo and Sapiro, Guillermo and Caselles, Vincent and Ballester, Coloma},
	month = jul,
	year = {2000},
	keywords = {anisotropic diffusion, image restoration, inpainting, isophotes},
	pages = {417--424},
}

@article{barnes_patchmatch_2009,
	title = {{PatchMatch}: a randomized correspondence algorithm for structural image editing},
	volume = {28},
	issn = {0730-0301, 1557-7368},
	shorttitle = {{PatchMatch}},
	url = {https://dl.acm.org/doi/10.1145/1531326.1531330},
	doi = {10.1145/1531326.1531330},
	abstract = {This paper presents interactive image editing tools using a new randomized algorithm for quickly ﬁnding approximate nearestneighbor matches between image patches. Previous research in graphics and vision has leveraged such nearest-neighbor searches to provide a variety of high-level digital image editing tools. However, the cost of computing a ﬁeld of such matches for an entire image has eluded previous efforts to provide interactive performance. Our algorithm offers substantial performance improvements over the previous state of the art (20-100x), enabling its use in interactive editing tools. The key insights driving the algorithm are that some good patch matches can be found via random sampling, and that natural coherence in the imagery allows us to propagate such matches quickly to surrounding areas. We offer theoretical analysis of the convergence properties of the algorithm, as well as empirical and practical evidence for its high quality and performance. This one simple algorithm forms the basis for a variety of tools – image retargeting, completion and reshufﬂing – that can be used together in the context of a high-level image editing application. Finally, we propose additional intuitive constraints on the synthesis process that offer the user a level of control unavailable in previous methods.},
	language = {en},
	number = {3},
	urldate = {2022-03-30},
	journal = {ACM Transactions on Graphics},
	author = {Barnes, Connelly and Shechtman, Eli and Finkelstein, Adam and Goldman, Dan B},
	month = jul,
	year = {2009},
	pages = {1--11},
}

@article{demir_patch-based_2018,
	title = {Patch-{Based} {Image} {Inpainting} with {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1803.07422},
	abstract = {Area of image inpainting over relatively large missing regions recently advanced substantially through adaptation of dedicated deep neural networks. However, current network solutions still introduce undesired artifacts and noise to the repaired regions. We present an image inpainting method that is based on the celebrated generative adversarial network (GAN) framework. The proposed PGGAN method includes a discriminator network that combines a global GAN (G-GAN) architecture with a patchGAN approach. PGGAN first shares network layers between G-GAN and patchGAN, then splits paths to produce two adversarial losses that feed the generator network in order to capture both local continuity of image texture and pervasive global features in images. The proposed framework is evaluated extensively, and the results including comparison to recent state-of-the-art demonstrate that it achieves considerable improvements on both visual and quantitative evaluations.},
	urldate = {2022-03-30},
	journal = {arXiv:1803.07422 [cs]},
	author = {Demir, Ugur and Unal, Gozde},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.07422},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{wang_linformer_2020,
	title = {Linformer: {Self}-{Attention} with {Linear} {Complexity}},
	shorttitle = {Linformer},
	url = {http://arxiv.org/abs/2006.04768},
	abstract = {Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses \$O(n{\textasciicircum}2)\$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from \$O(n{\textasciicircum}2)\$ to \$O(n)\$ in both time and space. The resulting linear transformer, the {\textbackslash}textit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
	urldate = {2022-03-21},
	journal = {arXiv:2006.04768 [cs, stat]},
	author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.04768
version: 3},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{shashikar_traffic_2017,
	title = {Traffic surveillance and anomaly detection using image processing},
	doi = {10.1109/ICIIP.2017.8313721},
	abstract = {Today there is a huge increase in the use of automobiles on roads and highways which has increased the concern to monitor and regulate the traffic on roads and highways. This has led to proposal of new and efficient traffic surveillance systems. This research focusses on detecting direction anomalies in the traffic surveillance videos using basic Image processing techniques and algorithms. The direction of the detected vehicle is checked per frame and if there is a case of direction anomaly, concerned authority is informed.},
	booktitle = {2017 {Fourth} {International} {Conference} on {Image} {Information} {Processing} ({ICIIP})},
	author = {Shashikar, Siddharth and Upadhyaya, Vikas},
	month = dec,
	year = {2017},
	keywords = {Alert generation, Anomaly detection, Background subtraction, Cameras, Direction anomaly detection, Direction detection, Information processing, Morphological operations, Object detection, Object tracking, Roads, Surveillance, Traffic control},
	pages = {1--6},
}

@article{ali_self-supervised_2020,
	title = {Self-{Supervised} {Representation} {Learning} for {Visual} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2006.09654},
	abstract = {Self-supervised learning allows for better utilization of unlabelled data. The feature representation obtained by self-supervision can be used in downstream tasks such as classification, object detection, segmentation, and anomaly detection. While classification, object detection, and segmentation have been investigated with self-supervised learning, anomaly detection needs more attention. We consider the problem of anomaly detection in images and videos, and present a new visual anomaly detection technique for videos. Numerous seminal and state-of-the-art self-supervised methods are evaluated for anomaly detection on a variety of image datasets. The best performing image-based self-supervised representation learning method is then used for video anomaly detection to see the importance of spatial features in visual anomaly detection in videos. We also propose a simple self-supervision approach for learning temporal coherence across video frames without the use of any optical flow information. At its core, our method identifies the frame indices of a jumbled video sequence allowing it to learn the spatiotemporal features of the video. This intuitive approach shows superior performance of visual anomaly detection compared to numerous methods for images and videos on UCF101 and ILSVRC2015 video datasets.},
	urldate = {2022-03-21},
	journal = {arXiv:2006.09654 [cs, eess]},
	author = {Ali, Rabia and Khan, Muhammad Umar Karim and Kyung, Chong Min},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.09654},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{li_cutpaste_2021-1,
	title = {{CutPaste}: {Self}-{Supervised} {Learning} for {Anomaly} {Detection} and {Localization}},
	shorttitle = {{CutPaste}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Li_CutPaste_Self-Supervised_Learning_for_Anomaly_Detection_and_Localization_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-03-21},
	author = {Li, Chun-Liang and Sohn, Kihyuk and Yoon, Jinsung and Pfister, Tomas},
	year = {2021},
	pages = {9664--9674},
}

@article{susto_anomaly_2017,
	series = {27th {International} {Conference} on {Flexible} {Automation} and {Intelligent} {Manufacturing}, {FAIM2017}, 27-30 {June} 2017, {Modena}, {Italy}},
	title = {Anomaly {Detection} {Approaches} for {Semiconductor} {Manufacturing}},
	volume = {11},
	issn = {2351-9789},
	url = {https://www.sciencedirect.com/science/article/pii/S2351978917305619},
	doi = {10.1016/j.promfg.2017.07.353},
	abstract = {Smart production monitoring is a crucial activity in advanced manufacturing for quality, control and maintenance purposes. Advanced Monitoring Systems aim to detect anomalies and trends; anomalies are data patterns that have different data characteristics from normal instances, while trends are tendencies of production to move in a particular direction over time. In this work, we compare state-of-the-art ML approaches (ABOD, LOF, onlinePCA and osPCA) to detect outliers and events in high-dimensional monitoring problems. The compared anomaly detection strategies have been tested on a real industrial dataset related to a Semiconductor Manufacturing Etching process.},
	language = {en},
	urldate = {2022-03-21},
	journal = {Procedia Manufacturing},
	author = {Susto, Gian Antonio and Terzi, Matteo and Beghi, Alessandro},
	month = jan,
	year = {2017},
	keywords = {Advanced Monitoring Systems, Anomaly Detection, Data Mining, Etching, Fault-diagnosis, Machine Learning, Optical Emission Spectroscopy, Semiconductor Manufacturing},
	pages = {2018--2024},
}

@article{han_madgan_2021,
	title = {{MADGAN}: unsupervised medical anomaly detection {GAN} using multiple adjacent brain {MRI} slice reconstruction},
	volume = {22},
	issn = {1471-2105},
	shorttitle = {{MADGAN}},
	url = {https://doi.org/10.1186/s12859-020-03936-1},
	doi = {10.1186/s12859-020-03936-1},
	abstract = {Unsupervised learning can discover various unseen abnormalities, relying on large-scale unannotated medical images of healthy subjects. Towards this, unsupervised methods reconstruct a 2D/3D single medical image to detect outliers either in the learned feature space or from high reconstruction loss. However, without considering continuity between multiple adjacent slices, they cannot directly discriminate diseases composed of the accumulation of subtle anatomical anomalies, such as Alzheimer’s disease (AD). Moreover, no study has shown how unsupervised anomaly detection is associated with either disease stages, various (i.e., more than two types of) diseases, or multi-sequence magnetic resonance imaging (MRI) scans.},
	number = {2},
	urldate = {2022-03-21},
	journal = {BMC Bioinformatics},
	author = {Han, Changhee and Rundo, Leonardo and Murao, Kohei and Noguchi, Tomoyuki and Shimahara, Yuki and Milacski, Zoltán Ádám and Koshino, Saori and Sala, Evis and Nakayama, Hideki and Satoh, Shin’ichi},
	month = apr,
	year = {2021},
	keywords = {Brain MRI reconstruction, Generative adversarial networks, Self-attention, Unsupervised anomaly detection, Various disease diagnosis},
	pages = {31},
}

@article{xue_gradient_2014-1,
	title = {Gradient {Magnitude} {Similarity} {Deviation}: {A} {Highly} {Efficient} {Perceptual} {Image} {Quality} {Index}},
	volume = {23},
	issn = {1941-0042},
	shorttitle = {Gradient {Magnitude} {Similarity} {Deviation}},
	doi = {10.1109/TIP.2013.2293423},
	abstract = {It is an important task to faithfully evaluate the perceptual quality of output images in many applications, such as image compression, image restoration, and multimedia streaming. A good image quality assessment (IQA) model should not only deliver high quality prediction accuracy, but also be computationally efficient. The efficiency of IQA metrics is becoming particularly important due to the increasing proliferation of high-volume visual data in high-speed networks. We present a new effective and efficient IQA model, called gradient magnitude similarity deviation (GMSD). The image gradients are sensitive to image distortions, while different local structures in a distorted image suffer different degrees of degradations. This motivates us to explore the use of global variation of gradient based local quality map for overall image quality prediction. We find that the pixel-wise gradient magnitude similarity (GMS) between the reference and distorted images combined with a novel pooling strategy-the standard deviation of the GMS map-can predict accurately perceptual image quality. The resulting GMSD algorithm is much faster than most state-of-the-art IQA methods, and delivers highly competitive prediction accuracy. MATLAB source code of GMSD can be downloaded at http://www4.comp.polyu.edu.hk/ cslzhang/IQA/GMSD/GMSD.htm.},
	number = {2},
	journal = {IEEE Transactions on Image Processing},
	author = {Xue, Wufeng and Zhang, Lei and Mou, Xuanqin and Bovik, Alan C.},
	month = feb,
	year = {2014},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Accuracy, Computational modeling, Degradation, Gradient magnitude similarity, Image coding, Image quality, Indexes, Nonlinear distortion, full reference, image quality assessment, standard deviation pooling},
	pages = {684--695},
}

@article{chen_visformer_2021,
	title = {Visformer: {The} {Vision}-friendly {Transformer}},
	shorttitle = {Visformer},
	url = {http://arxiv.org/abs/2104.12533},
	abstract = {The past year has witnessed the rapid development of applying the Transformer module to vision problems. While some researchers have demonstrated that Transformer-based models enjoy a favorable ability of fitting data, there are still growing number of evidences showing that these models suffer over-fitting especially when the training data is limited. This paper offers an empirical study by performing step-by-step operations to gradually transit a Transformer-based model to a convolution-based model. The results we obtain during the transition process deliver useful messages for improving visual recognition. Based on these observations, we propose a new architecture named Visformer, which is abbreviated from the `Vision-friendly Transformer'. With the same computational complexity, Visformer outperforms both the Transformer-based and convolution-based models in terms of ImageNet classification accuracy, and the advantage becomes more significant when the model complexity is lower or the training set is smaller. The code is available at https://github.com/danczs/Visformer.},
	urldate = {2021-11-15},
	journal = {arXiv:2104.12533 [cs]},
	author = {Chen, Zhengsu and Xie, Lingxi and Niu, Jianwei and Liu, Xuefeng and Wei, Longhui and Tian, Qi},
	month = sep,
	year = {2021},
	note = {arXiv: 2104.12533},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{steiner_how_2021,
	title = {How to train your {ViT}? {Data}, {Augmentation}, and {Regularization} in {Vision} {Transformers}},
	shorttitle = {How to train your {ViT}?},
	url = {http://arxiv.org/abs/2106.10270},
	abstract = {Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer's weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (``AugReg'' for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.},
	urldate = {2021-11-15},
	journal = {arXiv:2106.10270 [cs]},
	author = {Steiner, Andreas and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.10270},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{chen_when_2021,
	title = {When {Vision} {Transformers} {Outperform} {ResNets} without {Pre}-training or {Strong} {Data} {Augmentations}},
	url = {http://arxiv.org/abs/2106.01548},
	abstract = {Vision Transformers (ViTs) and MLPs signal further efforts on replacing hand-wired features or inductive biases with general-purpose neural architectures. Existing works empower the models by massive data, such as large-scale pre-training and/or repeated strong data augmentations, and still report optimization-related problems (e.g., sensitivity to initialization and learning rates). Hence, this paper investigates ViTs and MLP-Mixers from the lens of loss geometry, intending to improve the models' data efficiency at training and generalization at inference. Visualization and Hessian reveal extremely sharp local minima of converged models. By promoting smoothness with a recently proposed sharpness-aware optimizer, we substantially improve the accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning supervised, adversarial, contrastive, and transfer learning (e.g., +5.3{\textbackslash}\% and +11.0{\textbackslash}\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style preprocessing). We show that the improved smoothness attributes to sparser active neurons in the first few layers. The resultant ViTs outperform ResNets of similar size and throughput when trained from scratch on ImageNet without large-scale pre-training or strong data augmentations. They also possess more perceptive attention maps. Our model checkpoints are released at {\textbackslash}url\{https://github.com/google-research/vision\_transformer\}.},
	urldate = {2021-11-15},
	journal = {arXiv:2106.01548 [cs]},
	author = {Chen, Xiangning and Hsieh, Cho-Jui and Gong, Boqing},
	month = oct,
	year = {2021},
	note = {arXiv: 2106.01548},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{wu_fastformer_2021,
	title = {Fastformer: {Additive} {Attention} {Can} {Be} {All} {You} {Need}},
	shorttitle = {Fastformer},
	url = {http://arxiv.org/abs/2108.09084},
	abstract = {Transformer is a powerful model for text understanding. However, it is inefficient due to its quadratic complexity to input sequence length. Although there are many methods on Transformer acceleration, they are still either inefficient on long sequences or not effective enough. In this paper, we propose Fastformer, which is an efficient Transformer model based on additive attention. In Fastformer, instead of modeling the pair-wise interactions between tokens, we first use additive attention mechanism to model global contexts, and then further transform each token representation based on its interaction with global context representations. In this way, Fastformer can achieve effective context modeling with linear complexity. Extensive experiments on five datasets show that Fastformer is much more efficient than many existing Transformer models and can meanwhile achieve comparable or even better long text modeling performance.},
	urldate = {2021-11-05},
	journal = {arXiv:2108.09084 [cs]},
	author = {Wu, Chuhan and Wu, Fangzhao and Qi, Tao and Huang, Yongfeng and Xie, Xing},
	month = sep,
	year = {2021},
	note = {arXiv: 2108.09084},
	keywords = {Computer Science - Computation and Language},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2021-10-22},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ba_layer_2016,
	title = {Layer {Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	urldate = {2021-10-22},
	journal = {arXiv:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.06450},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2021-10-20},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{cho_learning_2014-1,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2021-10-20},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2021-10-20},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv: 1409.0473},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{xie_image_2012,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'12},
	title = {Image denoising and inpainting with deep neural networks},
	abstract = {We present a novel approach to low-level vision problems that combines sparse coding and deep networks pre-trained with denoising auto-encoder (DA). We propose an alternative training scheme that successfully adapts DA, originally designed for unsupervised feature learning, to the tasks of image denoising and blind inpainting. Our method's performance in the image denoising task is comparable to that of KSVD which is a widely used sparse coding technique. More importantly, in blind image inpainting task, the proposed method provides solutions to some complex problems that have not been tackled before. Specifically, we can automatically remove complex patterns like superimposed text from an image, rather than simple patterns like pixels missing at random. Moreover, the proposed method does not need the information regarding the region that requires inpainting to be given a priori. Experimental results demonstrate the effectiveness of the proposed method in the tasks of image denoising and blind inpainting. We also show that our new training scheme for DA is more effective and can improve the performance of unsupervised feature learning.},
	urldate = {2021-10-18},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 1},
	publisher = {Curran Associates Inc.},
	author = {Xie, Junyuan and Xu, Linli and Chen, Enhong},
	month = dec,
	year = {2012},
	pages = {341--349},
}

@article{wu_visual_2020,
	title = {Visual {Transformers}: {Token}-based {Image} {Representation} and {Processing} for {Computer} {Vision}},
	shorttitle = {Visual {Transformers}},
	url = {http://arxiv.org/abs/2006.03677},
	abstract = {Computer vision has achieved remarkable success by (a) representing images as uniformly-arranged pixel arrays and (b) convolving highly-localized features. However, convolutions treat all image pixels equally regardless of importance; explicitly model all concepts across all images, regardless of content; and struggle to relate spatially-distant concepts. In this work, we challenge this paradigm by (a) representing images as semantic visual tokens and (b) running transformers to densely model token relationships. Critically, our Visual Transformer operates in a semantic token space, judiciously attending to different image parts based on context. This is in sharp contrast to pixel-space transformers that require orders-of-magnitude more compute. Using an advanced training recipe, our VTs significantly outperform their convolutional counterparts, raising ResNet accuracy on ImageNet top-1 by 4.6 to 7 points while using fewer FLOPs and parameters. For semantic segmentation on LIP and COCO-stuff, VT-based feature pyramid networks (FPN) achieve 0.35 points higher mIoU while reducing the FPN module's FLOPs by 6.5x.},
	urldate = {2021-10-18},
	journal = {arXiv:2006.03677 [cs, eess]},
	author = {Wu, Bichen and Xu, Chenfeng and Dai, Xiaoliang and Wan, Alvin and Zhang, Peizhao and Yan, Zhicheng and Tomizuka, Masayoshi and Gonzalez, Joseph and Keutzer, Kurt and Vajda, Peter},
	month = nov,
	year = {2020},
	note = {arXiv: 2006.03677},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Transformer},
}

@article{katharopoulos_transformers_2020,
	title = {Transformers are {RNNs}: {Fast} {Autoregressive} {Transformers} with {Linear} {Attention}},
	shorttitle = {Transformers are {RNNs}},
	url = {http://arxiv.org/abs/2006.16236},
	abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textasciicircum}2{\textbackslash}right)\$ to \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textbackslash}right)\$, where \$N\$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
	urldate = {2021-10-18},
	journal = {arXiv:2006.16236 [cs, stat]},
	author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, François},
	month = aug,
	year = {2020},
	note = {arXiv: 2006.16236},
	keywords = {Attention, Computer Science - Machine Learning, Statistics - Machine Learning, Transformer},
}

@article{yeh_semantic_2017,
	title = {Semantic {Image} {Inpainting} with {Deep} {Generative} {Models}},
	url = {http://arxiv.org/abs/1607.07539},
	abstract = {Semantic image inpainting is a challenging task where large missing regions have to be filled based on the available visual data. Existing methods which extract information from only a single image generally produce unsatisfactory results due to the lack of high level context. In this paper, we propose a novel method for semantic image inpainting, which generates the missing content by conditioning on the available data. Given a trained generative model, we search for the closest encoding of the corrupted image in the latent image manifold using our context and prior losses. This encoding is then passed through the generative model to infer the missing content. In our method, inference is possible irrespective of how the missing content is structured, while the state-of-the-art learning based method requires specific information about the holes in the training phase. Experiments on three datasets show that our method successfully predicts information in large missing regions and achieves pixel-level photorealism, significantly outperforming the state-of-the-art methods.},
	urldate = {2021-10-18},
	journal = {arXiv:1607.07539 [cs]},
	author = {Yeh, Raymond A. and Chen, Chen and Lim, Teck Yian and Schwing, Alexander G. and Hasegawa-Johnson, Mark and Do, Minh N.},
	month = jul,
	year = {2017},
	note = {arXiv: 1607.07539},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Inpainting},
}

@article{doersch_multi-task_2017,
	title = {Multi-task {Self}-{Supervised} {Visual} {Learning}},
	url = {http://arxiv.org/abs/1708.07860},
	abstract = {We investigate methods for combining multiple self-supervised tasks--i.e., supervised tasks where data can be collected without manual labeling--in order to train a single visual representation. First, we provide an apples-to-apples comparison of four different self-supervised tasks using the very deep ResNet-101 architecture. We then combine tasks to jointly train a network. We also explore lasso regularization to encourage the network to factorize the information in its representation, and methods for "harmonizing" network inputs in order to learn a more unified representation. We evaluate all methods on ImageNet classification, PASCAL VOC detection, and NYU depth prediction. Our results show that deeper networks work better, and that combining tasks--even via a naive multi-head architecture--always improves performance. Our best joint network nearly matches the PASCAL performance of a model pre-trained on ImageNet classification, and matches the ImageNet network on NYU depth prediction.},
	urldate = {2021-10-18},
	journal = {arXiv:1708.07860 [cs]},
	author = {Doersch, Carl and Zisserman, Andrew},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.07860},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Self-supervised},
}

@article{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2021-10-18},
	journal = {arXiv:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv: 2010.11929},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Transformer},
}

@article{yu_diverse_2021-1,
	title = {Diverse {Image} {Inpainting} with {Bidirectional} and {Autoregressive} {Transformers}},
	url = {http://arxiv.org/abs/2104.12335},
	abstract = {Image inpainting is an underdetermined inverse problem, which naturally allows diverse contents to fill up the missing or corrupted regions realistically. Prevalent approaches using convolutional neural networks (CNNs) can synthesize visually pleasant contents, but CNNs suffer from limited perception fields for capturing global features. With image-level attention, transformers enable to model long-range dependencies and generate diverse contents with autoregressive modeling of pixel-sequence distributions. However, the unidirectional attention in autoregressive transformers is suboptimal as corrupted image regions may have arbitrary shapes with contexts from any direction. We propose BAT-Fill, an innovative image inpainting framework that introduces a novel bidirectional autoregressive transformer (BAT) for image inpainting. BAT utilizes the transformers to learn autoregressive distributions, which naturally allows the diverse generation of missing contents. In addition, it incorporates the masked language model like BERT, which enables bidirectionally modeling of contextual information of missing regions for better image completion. Extensive experiments over multiple datasets show that BAT-Fill achieves superior diversity and fidelity in image inpainting qualitatively and quantitatively.},
	urldate = {2021-10-18},
	journal = {arXiv:2104.12335 [cs]},
	author = {Yu, Yingchen and Zhan, Fangneng and Wu, Rongliang and Pan, Jianxiong and Cui, Kaiwen and Lu, Shijian and Ma, Feiying and Xie, Xuansong and Miao, Chunyan},
	month = jun,
	year = {2021},
	note = {arXiv: 2104.12335},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Inpainting, Transformer},
}

@article{zenati_efficient_2019,
	title = {Efficient {GAN}-{Based} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1802.06222},
	abstract = {Generative adversarial networks (GANs) are able to model the complex highdimensional distributions of real-world data, which suggests they could be effective for anomaly detection. However, few works have explored the use of GANs for the anomaly detection task. We leverage recently developed GAN models for anomaly detection, and achieve state-of-the-art performance on image and network intrusion datasets, while being several hundred-fold faster at test time than the only published GAN-based method.},
	urldate = {2021-10-18},
	journal = {arXiv:1802.06222 [cs, stat]},
	author = {Zenati, Houssam and Foo, Chuan Sheng and Lecouat, Bruno and Manek, Gaurav and Chandrasekhar, Vijay Ramaseshan},
	month = may,
	year = {2019},
	note = {arXiv: 1802.06222},
	keywords = {Anomaly detection, Computer Science - Machine Learning, GAN, Statistics - Machine Learning},
}

@article{ngo_fence_2019,
	title = {Fence {GAN}: {Towards} {Better} {Anomaly} {Detection}},
	shorttitle = {Fence {GAN}},
	url = {http://arxiv.org/abs/1904.01209},
	abstract = {Anomaly detection is a classical problem where the aim is to detect anomalous data that do not belong to the normal data distribution. Current state-of-the-art methods for anomaly detection on complex high-dimensional data are based on the generative adversarial network (GAN). However, the traditional GAN loss is not directly aligned with the anomaly detection objective: it encourages the distribution of the generated samples to overlap with the real data and so the resulting discriminator has been found to be ineffective as an anomaly detector. In this paper, we propose simple modifications to the GAN loss such that the generated samples lie at the boundary of the real data distribution. With our modified GAN loss, our anomaly detection method, called Fence GAN (FGAN), directly uses the discriminator score as an anomaly threshold. Our experimental results using the MNIST, CIFAR10 and KDD99 datasets show that Fence GAN yields the best anomaly classification accuracy compared to state-of-the-art methods.},
	urldate = {2021-10-18},
	journal = {arXiv:1904.01209 [cs, stat]},
	author = {Ngo, Cuong Phuc and Winarto, Amadeus Aristo and Li, Connie Kou Khor and Park, Sojeong and Akram, Farhan and Lee, Hwee Kuan},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.01209},
	keywords = {Anomaly detection, Computer Science - Machine Learning, GAN, Statistics - Machine Learning},
}

@article{erfani_high-dimensional_2016,
	title = {High-dimensional and large-scale anomaly detection using a linear one-class {SVM} with deep learning},
	volume = {58},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320316300267},
	doi = {10.1016/j.patcog.2016.03.028},
	abstract = {High-dimensional problem domains pose significant challenges for anomaly detection. The presence of irrelevant features can conceal the presence of anomalies. This problem, known as the ‘curse of dimensionality’, is an obstacle for many anomaly detection techniques. Building a robust anomaly detection model for use in high-dimensional spaces requires the combination of an unsupervised feature extractor and an anomaly detector. While one-class support vector machines are effective at producing decision surfaces from well-behaved feature vectors, they can be inefficient at modelling the variation in large, high-dimensional datasets. Architectures such as deep belief networks (DBNs) are a promising technique for learning robust features. We present a hybrid model where an unsupervised DBN is trained to extract generic underlying features, and a one-class SVM is trained from the features learned by the DBN. Since a linear kernel can be substituted for nonlinear ones in our hybrid model without loss of accuracy, our model is scalable and computationally efficient. The experimental results show that our proposed model yields comparable anomaly detection performance with a deep autoencoder, while reducing its training and testing time by a factor of 3 and 1000, respectively.},
	language = {en},
	urldate = {2021-10-18},
	journal = {Pattern Recognition},
	author = {Erfani, Sarah M. and Rajasegarar, Sutharshan and Karunasekera, Shanika and Leckie, Christopher},
	month = oct,
	year = {2016},
	keywords = {Anomaly detection, Deep belief net, Deep learning, Feature extraction, High-dimensional data, One-class SVM, Outlier detection},
	pages = {121--134},
}

@inproceedings{loganathan_sequence_2018,
	title = {Sequence to {Sequence} {Pattern} {Learning} {Algorithm} for {Real}-{Time} {Anomaly} {Detection} in {Network} {Traffic}},
	doi = {10.1109/CCECE.2018.8447597},
	abstract = {Network intrusions can be modeled as anomalies in network traffic in which the expected order of packets and their attributes deviate from regular traffic. Algorithms that predict the next sequence of events based on previous sequences are a promising avenue for detecting such anomalies. In this paper, we present a novel multi-attribute model for predicting a network packet sequence based on previous packets using a sequence-to-sequence (Seq2Seq) encoder-decoder model. This model is trained on an attack-free dataset to learn the normal sequence of packets in TCP connections and then it is used to detect anomalous packets in TCP traffic. We show that in DARPA 1999 dataset, the proposed multi-attribute Seq2Seq model detects anomalous raw TCP packets which are part of intrusions with 97 \% accuracy. Also, it can detect selected intrusions in real-time with 100\% accuracy and outperforms existing algorithms based on recurrent neural network models such as LSTM.},
	booktitle = {2018 {IEEE} {Canadian} {Conference} on {Electrical} {Computer} {Engineering} ({CCECE})},
	author = {Loganathan, Gobinath and Samarabandu, Jagath and Wang, Xianbin},
	month = may,
	year = {2018},
	note = {ISSN: 2576-7046},
	keywords = {Anomaly detection, Computational modeling, Decoding, Predictive models, Real-time systems, Training, Videos},
	pages = {1--4},
}

@book{mohri_foundations_2012,
	address = {Cambridge, MA, USA},
	series = {Adaptive {Computation} and {Machine} {Learning} series},
	title = {Foundations of {Machine} {Learning}},
	isbn = {978-0-262-01825-8},
	abstract = {Fundamental topics in machine learning are presented along with theoretical and conceptual tools for the discussion and proof of algorithms.},
	language = {en},
	publisher = {MIT Press},
	author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
	editor = {Bach, Francis},
	month = aug,
	year = {2012},
}

@article{prakash_multi-modal_2021,
	title = {Multi-{Modal} {Fusion} {Transformer} for {End}-to-{End} {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2104.09224},
	abstract = {How should representations from complementary sensors be integrated for autonomous driving? Geometry-based sensor fusion has shown great promise for perception tasks such as object detection and motion forecasting. However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in traffic light state can affect the behavior of a vehicle geometrically distant from that traffic light. Geometry alone may therefore be insufficient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that imitation learning policies based on existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global contextual reasoning, such as handling traffic oncoming from multiple directions at uncontrolled intersections. Therefore, we propose TransFuser, a novel Multi-Modal Fusion Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efficacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reducing collisions by 76\% compared to geometry-based fusion.},
	urldate = {2021-10-18},
	journal = {arXiv:2104.09224 [cs]},
	author = {Prakash, Aditya and Chitta, Kashyap and Geiger, Andreas},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.09224},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{chen_generative_nodate,
	title = {Generative {Pretraining} from {Pixels}},
	abstract = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we ﬁnd that a GPT-2 scale model learns strong image representations as measured by linear probing, ﬁne-tuning, and low-data classiﬁcation. On CIFAR-10, we achieve 96.3\% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0\% accuracy with full ﬁnetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0\% top-1 accuracy on a linear probe of our features.},
	language = {en},
	author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeff and Jun, Heewoo and Dhariwal, Prafulla and Luan, David and Sutskever, Ilya},
	pages = {12},
}

@article{tsai_autoencoder-based_2021,
	title = {Autoencoder-based anomaly detection for surface defect inspection},
	volume = {48},
	issn = {1474-0346},
	url = {https://www.sciencedirect.com/science/article/pii/S1474034621000276},
	doi = {10.1016/j.aei.2021.101272},
	abstract = {In this paper, the unsupervised autoencoder learning for automated defect detection in manufacturing is evaluated, where only the defect-free samples are required for the model training. The loss function of a Convolutional Autoencoder (CAE) model only aims at minimizing the reconstruction errors, and makes the representative features widely spread. The proposed CAE in this study incorporates a regularization that improves the feature distribution of defect-free samples within a tight range. It makes the representative feature vectors of all training samples as close as possible to the mean feature vector so that a defect sample in the evaluation stage can generate a distinct distance from the trained center of defect-free samples. The proposed CAE model with regularizations has been tested on a variety of material surfaces, including textural and patterned surfaces in images. The experimental results reveal that the proposed CAE with regularizations significantly outperforms the conventional CAE for defect detection applications in the industry.},
	language = {en},
	urldate = {2021-10-18},
	journal = {Advanced Engineering Informatics},
	author = {Tsai, Du-Ming and Jen, Po-Hao},
	month = apr,
	year = {2021},
	keywords = {Anomaly detection, Autoencoders, Defect inspection, Machine vision},
	pages = {101272},
}

@article{xie_semisupervised_2021,
	title = {Semisupervised {Training} of {Deep} {Generative} {Models} for {High}-{Dimensional} {Anomaly} {Detection}},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2021.3095150},
	abstract = {Abnormal behaviors in industrial systems may be early warnings on critical events that may cause severe damages to facilities and security. Thus, it is important to detect abnormal behaviors accurately and timely. However, the anomaly detection problem is hard to solve in practice, mainly due to the rareness and the expensive cost to get the labels of the anomalies. Deep generative models parameterized by neural networks have achieved state-of-the-art performance in practice for many unsupervised and semisupervised learning tasks. We present a new deep generative model, Latent Enhanced regression/classification Deep Generative Model (LEDGM), for the anomaly detection problem with multidimensional data. Instead of using two-stage decoupled models, we adopt an end-to-end learning paradigm. Instead of conditioning the latent on the class label, LEDGM conditions the label prediction on the learned latent so that the optimization goal is more in favor of better anomaly detection than better reconstruction that the previously proposed deep generative models have been trained for. Experimental results on several synthetic and real-world small- and large-scale datasets demonstrate that LEDGM can achieve improved anomaly detection performance on multidimensional data with very sparse labels. The results also suggest that both labeled anomalies and labeled normal are valuable for semisupervised learning. Generally, our results show that better performance can be achieved with more labeled data. The ablation experiments show that both the original input and the learned latent provide meaningful information for LEDGM to achieve high performance.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Xie, Qin and Zhang, Peng and Yu, Boseon and Choi, Jaesik},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Anomaly detection, Data models, Generative adversarial networks, Generators, Semisupervised learning, Training, Unsupervised learning, deep generative models, semisupervised learning, variational autoencoder (VAE).},
	pages = {1--10},
}

@inproceedings{luong_effective_2015,
	address = {Lisbon, Portugal},
	title = {Effective {Approaches} to {Attention}-based {Neural} {Machine} {Translation}},
	url = {http://aclweb.org/anthology/D15-1166},
	doi = {10.18653/v1/D15-1166},
	language = {en},
	urldate = {2021-10-18},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Luong, Thang and Pham, Hieu and Manning, Christopher D.},
	year = {2015},
	pages = {1412--1421},
}

@article{pirnay_inpainting_2021,
	title = {Inpainting {Transformer} for {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2104.13897},
	abstract = {Anomaly detection in computer vision is the task of identifying images which deviate from a set of normal images. A common approach is to train deep convolutional autoencoders to inpaint covered parts of an image and compare the output with the original image. By training on anomaly-free samples only, the model is assumed to not being able to reconstruct anomalous regions properly. For anomaly detection by inpainting we suggest it to be beneficial to incorporate information from potentially distant regions. In particular we pose anomaly detection as a patch-inpainting problem and propose to solve it with a purely self-attention based approach discarding convolutions. The proposed Inpainting Transformer (InTra) is trained to inpaint covered patches in a large sequence of image patches, thereby integrating information across large regions of the input image. When training from scratch, in comparison to other methods not using extra training data, InTra achieves results on par with the current state-of-the-art on the MVTec AD dataset for detection and surpassing them on segmentation.},
	urldate = {2021-10-18},
	journal = {arXiv:2104.13897 [cs]},
	author = {Pirnay, Jonathan and Chai, Keng},
	month = sep,
	year = {2021},
	note = {arXiv: 2104.13897},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2021-10-18},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Attention, Computer Science - Computation and Language, Computer Science - Machine Learning, Transformer},
}

@misc{brownlee_what_2017,
	title = {What is {Teacher} {Forcing} for {Recurrent} {Neural} {Networks}?},
	url = {https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/},
	abstract = {Teacher forcing is a method for quickly and efficiently training recurrent neural network models that use the ground truth from […]},
	language = {en-US},
	urldate = {2021-10-18},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = dec,
	year = {2017},
	keywords = {Teacher forcing},
}
